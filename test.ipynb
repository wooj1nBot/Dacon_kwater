{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ValidTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, edge_index: List, window_size: int):\n",
    "        super(ValidTimeSeriesDataset, self).__init__(None, None, None)\n",
    "        self.edge_index = edge_index\n",
    "        self.window_size = window_size\n",
    "        self._read_data(df)\n",
    "        self._get_edges()\n",
    "        self._set_targets_and_features()\n",
    "\n",
    "    def _read_data(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.pump_names = df.filter(regex='^P\\\\d+$').columns.tolist()\n",
    "        self.flags_names = df.filter(regex=r'^P\\d+_flag$').columns.tolist()\n",
    "        self.flux_names = df.filter(regex='^Q\\\\d+$').columns.tolist()\n",
    "\n",
    "        self.accident_labels = df['anomaly'].values.astype(np.int8)\n",
    "        self.press_values = torch.from_numpy(df[self.pump_names].values.astype(np.float32)) # (T, p_count)\n",
    "        self.flux_values = torch.from_numpy(df[self.flux_names].values.astype(np.float32)) # (T, f_count)\n",
    "        self.labels = torch.from_numpy(df[self.flags_names].values.astype(np.float32)) # (T, p_count)\n",
    "\n",
    "        potential_starts = torch.arange(0, len(self.df) - self.window_size)\n",
    "        self.valid_starts = [\n",
    "            idx for idx in potential_starts \n",
    "            if np.all(self.accident_labels[idx:idx+self.window_size] == 0)\n",
    "        ]\n",
    "\n",
    "    def _get_edges(self):\n",
    "        self._edges = torch.tensor(self.edge_index, dtype=torch.long).T\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.valid_starts)\n",
    "    \n",
    "    def _normalize_window(self, window: torch.Tensor):\n",
    "        # window => (w, n) => (n, w)\n",
    "        window = window.T\n",
    "        min_val = window.min(dim=1, keepdim=True)[0]\n",
    "        max_val = window.max(dim=1, keepdim=True)[0]\n",
    "        # max_val - min_val이 0인 경우를 방지하기 위해 1e-10을 더함\n",
    "        normalized_x = (window - min_val) / (max_val - min_val + 1e-10)\n",
    "        return normalized_x\n",
    "    \n",
    "    def _set_targets_and_features(self):\n",
    "        valid_press_values = torch.stack([self._normalize_window(self.press_values[idx:idx+self.window_size]) for idx in self.valid_starts]) # (t, p_count, w)\n",
    "        valid_flux_values = torch.stack([self._normalize_window(self.flux_values[idx:idx+self.window_size]) for idx in self.valid_starts]) # (t, f_count, w)\n",
    "        self.features = valid_press_values\n",
    "    \n",
    "    def get(self, i):\n",
    "        data = Data(x=self.features[i], edge_index=self._edges)\n",
    "        return data\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, edge_index: List, window_size: int):\n",
    "        super(TimeSeriesDataset, self).__init__(None, None, None)\n",
    "        self.edge_index = edge_index\n",
    "        self.window_size = window_size\n",
    "        self._read_data(df)\n",
    "        self._get_edges()\n",
    "\n",
    "    def _read_data(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.pump_names = df.filter(regex='^P\\\\d+$').columns.tolist()\n",
    "        self.flags_names = df.filter(regex=r'^P\\d+_flag$').columns.tolist()\n",
    "        self.flux_names = df.filter(regex='^Q\\\\d+$').columns.tolist()\n",
    "\n",
    "        self.accident_labels = df['anomaly'].values.astype(np.int8)\n",
    "        self.press_values = torch.from_numpy(df[self.pump_names].values.astype(np.float32)) # (T, p_count)\n",
    "        self.flux_values = torch.from_numpy(df[self.flux_names].values.astype(np.float32)) # (T, f_count)\n",
    "        self.labels = torch.from_numpy(df[self.flags_names].values.astype(np.float32)) # (T, p_count)\n",
    "\n",
    "    def _get_edges(self):\n",
    "        self._edges = torch.tensor(self.edge_index, dtype=torch.long).T\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.press_values) - self.window_size\n",
    "    \n",
    "    def _normalize_window(self, window: torch.Tensor):\n",
    "        # window => (w, n) => (n, w)\n",
    "        window = window.T\n",
    "        mins = torch.min(window, dim=1, keepdim=True).values\n",
    "        maxs = torch.max(window, dim=1, keepdim=True).values\n",
    "        normalized_window = (window - mins) / (maxs - mins)\n",
    "        return normalized_window\n",
    "      \n",
    "    \n",
    "    def get(self, i):\n",
    "        window = self._normalize_window(self.press_values[i:i+self.window_size]) # (n, w)\n",
    "        data = Data(x=window, edge_index=self._edges, y=self.labels[i:i+self.window_size])\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_A = [\n",
    "    (0, 4), (0, 5), (0, 7), (0, 8), (0, 6), (0, 11), (0, 9), (0, 18), (0, 25),\n",
    "    (1, 3),\n",
    "    (2, 4), (2, 5), (2, 7), (2, 8), (2, 6), (2, 11), (2, 9), (2, 18), (2, 25),\n",
    "    (3, 4), (3, 5), (3, 7), (3, 8), (3, 6), (3, 11), (3, 9), (3, 18), (3, 25),\n",
    "    (4, 7), (4, 8), (4, 6), (4, 11), (4, 9), (4, 18), (4, 25),\n",
    "    (5, 7), (5, 8), (5, 6), (5, 11), (5, 9), (5, 18), (5, 25),\n",
    "    (6, 8),\n",
    "    (7, 11), (7, 9), (7, 18), (7, 25),\n",
    "    (8, 11), (8, 9), (8, 18), (8, 25),\n",
    "    (9, 10),\n",
    "    (10, 13),\n",
    "    (11, 12), (11, 17),\n",
    "    (12, 13), (12, 17),\n",
    "    (13, 12), (13, 14),\n",
    "    (14, 15),\n",
    "    (15, 16), (15, 18),\n",
    "    (17, 19), (17, 18),\n",
    "    (18, 17), (18, 21), (18, 25),\n",
    "    (19, 20),\n",
    "    (20, 23),\n",
    "    (21, 22),\n",
    "    (22, 24)\n",
    "]\n",
    "\n",
    "window_size = 60*24*7 # 10080\n",
    "dataframe = pd.read_csv('/home/wujin/workspace/competition2/datasets/train/TRAIN_A.csv')\n",
    "dataset = ValidTimeSeriesDataset(df=dataframe, edge_index=edge_index_A, window_size=window_size)\n",
    "test_dataset = TimeSeriesDataset(df=dataframe, edge_index=edge_index_A, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26, 10080])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1].x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(len(dataset) * 0.8)  # 80%를 훈련 데이터로\n",
    "val_size = len(dataset) - train_size  # 나머지 20%를 검증 데이터로\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 425/425 [00:38<00:00, 11.01batch/s, loss=0.707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Average Train Loss: 1.67423802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 107/107 [00:03<00:00, 28.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Average Eval Loss: 0.69580389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 425/425 [00:38<00:00, 11.03batch/s, loss=0.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Average Train Loss: 0.70213697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 107/107 [00:03<00:00, 28.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Average Eval Loss: 0.69391456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 425/425 [00:38<00:00, 11.03batch/s, loss=0.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Average Train Loss: 0.69622285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 107/107 [00:03<00:00, 28.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Average Eval Loss: 0.69354670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 425/425 [00:38<00:00, 11.04batch/s, loss=0.695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Average Train Loss: 0.69531616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 107/107 [00:03<00:00, 28.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Average Eval Loss: 0.69342403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 425/425 [00:38<00:00, 11.07batch/s, loss=0.695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Average Train Loss: 0.69496054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 107/107 [00:03<00:00, 27.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Average Eval Loss: 0.69346295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 425/425 [00:38<00:00, 11.03batch/s, loss=0.695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Average Train Loss: 0.69473756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 107/107 [00:03<00:00, 28.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Average Eval Loss: 0.69334110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 425/425 [00:38<00:00, 11.05batch/s, loss=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Average Train Loss: 0.69457279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 107/107 [00:03<00:00, 28.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Average Eval Loss: 0.69334203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 425/425 [00:38<00:00, 11.05batch/s, loss=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Average Train Loss: 0.69443695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 107/107 [00:03<00:00, 28.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Average Eval Loss: 0.69333002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 425/425 [00:38<00:00, 11.04batch/s, loss=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Average Train Loss: 0.69433001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 107/107 [00:03<00:00, 28.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Average Eval Loss: 0.69331609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 425/425 [00:38<00:00, 11.06batch/s, loss=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Average Train Loss: 0.69423732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 107/107 [00:03<00:00, 28.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Average Eval Loss: 0.69328417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 425/425 [00:38<00:00, 11.04batch/s, loss=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Average Train Loss: 0.69415701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 107/107 [00:03<00:00, 28.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Average Eval Loss: 0.69326186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30:  93%|█████████▎| 395/425 [00:36<00:02, 10.97batch/s, loss=0.694]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     23\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 24\u001b[0m         epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m         t\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     27\u001b[0m avg_epoch_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from models.model import GraphSAGEVAE\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model = GraphSAGEVAE(in_channels=window_size, hidden_channels=512, out_channels=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 30\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\") as t:\n",
    "        for batch in t:\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            loss, recon_loss, kl_loss = model(x, edge_index)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}, Average Train Loss: {avg_epoch_loss:.8f}\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    for batch in tqdm(val_loader, desc=\"Eval\"):\n",
    "            x = batch.x.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            loss, recon_loss, kl_loss = model(x, edge_index)\n",
    "            eval_loss += loss.item()\n",
    "    \n",
    "    avg_eval_loss = eval_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}, Average Eval Loss: {avg_eval_loss:.8f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test:   0%|          | 0/532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645120 1664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (26) must match the size of tensor b (1664) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(node_scores))\n\u001b[1;32m     43\u001b[0m anomaly_labels \u001b[38;5;241m=\u001b[39m (node_scores \u001b[38;5;241m>\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mint()\n\u001b[0;32m---> 44\u001b[0m total_matched_abnormal \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43manomaly_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     45\u001b[0m total_false_positives \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (anomaly_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     46\u001b[0m total_pred_abnormal \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (anomaly_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (26) must match the size of tensor b (1664) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "threshold_percentile = 95\n",
    "total_matched_abnormal = 0\n",
    "total_false_positives = 0\n",
    "total_pred_abnormal = 0\n",
    "total_gt_abnormal = 0\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(test_loader, desc=\"Test\"):\n",
    "        x = batch.x.to(device)\n",
    "        edge_index = batch.edge_index.to(device)\n",
    "        z, mean, log_std = model(x, edge_index, neg_edge_index)\n",
    "        adj = to_dense_adj(edge_index, max_num_nodes=batch.num_nodes).squeeze(0)\n",
    "        with torch.no_grad():\n",
    "            # 모든 엣지 쌍에 대해 내적 계산\n",
    "            # z: [N, d], z @ z.T: [N, N]\n",
    "            pred_adj = torch.sigmoid(torch.matmul(z, z.t()))\n",
    "        \n",
    "        # 노드 수준의 재구성 오류 계산\n",
    "        # 각 노드 i에 대해 실제 인접 벡터 adj[i], 예측 pred_adj[i] 비교\n",
    "        # 여기서는 Binary Cross Entropy 기반으로 계산 가능\n",
    "        print(batch.num_nodes)\n",
    "        node_scores = []\n",
    "        for i in range(batch.num_nodes):\n",
    "            # 실제 값 (adj의 i번째 row): 0 또는 1\n",
    "            # 예측 값 (pred_adj의 i번째 row): 0과 1 사이의 확률값\n",
    "            # BCE = - [y*log(p) + (1-y)*log(1-p)] 의 평균\n",
    "            y = adj[i]\n",
    "            p = pred_adj[i]\n",
    "\n",
    "            # 노드 i에 연결된 엣지만 고려할 수도 있고(양성 중심),\n",
    "            # 여기서는 모든 노드 쌍을 동일 비중으로 사용\n",
    "            bce = -(y * torch.log(p + 1e-15) + (1 - y)*torch.log(1 - p + 1e-15))\n",
    "            node_score = bce.mean().item()\n",
    "            node_scores.append(node_score)\n",
    "\n",
    "        node_scores = torch.tensor(node_scores)\n",
    "        threshold = torch.quantile(node_scores, threshold_percentile / 100.0).item()\n",
    "        \n",
    "        # 이상치로 간주되는 노드 식별\n",
    "        labels = batch.y\n",
    "        anomaly_labels = (node_scores > threshold).int()\n",
    "        total_matched_abnormal += ((labels == 1) & (anomaly_labels == 1)).sum().item()\n",
    "        total_false_positives += ((labels == 0) & (anomaly_labels == 1)).sum().item()\n",
    "        total_pred_abnormal += (anomaly_labels == 1).sum().item()\n",
    "        total_gt_abnormal += (labels == 1).sum().item()\n",
    "\n",
    "precision = total_matched_abnormal / (total_pred_abnormal + total_false_positives)\n",
    "recall = total_matched_abnormal / total_gt_abnormal\n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "print(\"f1 score: \", f1_score)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1, 0, 0, 1, 1]) # 실제\n",
    "b = torch.Tensor([1, 1, 0, 0, 1]) # 예측\n",
    "\n",
    "matched_abnormal = ((a == 1) & (b == 1)).sum().item()\n",
    "false_positives = ((a == 0) & (b == 1)).sum().item()\n",
    "pred_abnormal = (b == 1).sum().item()\n",
    "gt_abnormal = (a == 1).sum().item()\n",
    "print(gt_abnormal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
